import { chromium } from "playwright"; import fs from "fs-extra"; const browser = await chromium.launch(); const page = await browser.newPage(); const visitedUrls = new Set(); const scrapedData = []; async function extractLinks(url) { await page.goto(url, { waitUntil: "networkidle" }); await page.waitForTimeout(3000); const links = await page.evaluate(() => { return Array.from(document.querySelectorAll("a[href]")).map(a => a.href).filter(href => href.includes("Nethelp/") && href.endsWith(".htm")); }); return [...new Set(links)]; } async function scrapePage(url) { if (visitedUrls.has(url)) return null; visitedUrls.add(url); console.log("Skrapar:", url); try { await page.goto(url, { waitUntil: "networkidle" }); await page.waitForTimeout(2000); const data = await page.evaluate(() => ({ title: document.title, content: document.querySelector("#c1topic")?.innerText || document.body.innerText, url: window.location.href, timestamp: new Date().toISOString() })); console.log("Skrapade:", data.title); return data; } catch (error) { console.error("Fel:", error.message); return null; } } const startUrl = "https://nethelpv2.flexhosting.se/SVE/#!Nethelp/623.htm"; const firstPage = await scrapePage(startUrl); if (firstPage) scrapedData.push(firstPage); const allLinks = await extractLinks(startUrl); console.log("Hittade", allLinks.length, "lÃ¤nkar"); for (const url of allLinks) { const pageData = await scrapePage(url); if (pageData) scrapedData.push(pageData); await new Promise(r => setTimeout(r, 1000)); } await fs.ensureDir("./scraped-data-complete"); await fs.writeJson("./scraped-data-complete/all-data.json", scrapedData, { spaces: 2 }); const allText = scrapedData.map(p => `=== ${p.title} ===
${p.content}

`).join(""); await fs.writeFile("./scraped-data-complete/all-text.txt", allText); console.log("Klar! Skrapade", scrapedData.length, "sidor"); await browser.close();
